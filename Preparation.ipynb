{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a32e0c",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "* The translation tasks have been completed and all translations are now stored in certain hierarchy within the `tasks` folder, which was backed up with the logs associated with it. At the time of writing, it is unclear what can be made public or not, the logs contain IP addresses in some cases due to how the errors were logged, the translations could be uplouded to Google Drive and provided to everyone. For now, we plan to only provide them to evaluators of this project. \n",
    "\n",
    "* In this notebook, we re-structure the translation data to make it easier to work with them (align & evaluate)\n",
    "* First, we extract time information found in proc1-3 logs as the start times stored proc1-3 JSONL files include each time OpenAI's code triggered an automatic retry.\n",
    "* Second, we rename filenames and move all translations into a single folder, preserve hierarchy by using prefixes and omit procedure-related information, as it is not required for alignment and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc926c",
   "metadata": {},
   "source": [
    "## Getting Preciser Timestamps\n",
    "* In proc 1 to 3, there were cases where GPT4.1 took longer than expected. \n",
    "* The structured logs in the JSONL file stored the start time when the method that calls the API was called and the end time when the response was received. \n",
    "* However, this did not account for the case of OpenAI's code doing the retries, so the retries where included in the time calculation.\n",
    "* In this notebook, we use the logs to extract start and end times that are more in line with the real translation time\n",
    "* We do this by looking at the `DEBUG` logs created by OpenAI's code, they contain an entry of when exactly a request was sent.\n",
    "* The end time we can obtain by our own logs, looking for `Translated X sents for src-tgt` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2895f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 2025-05-09 13:18:06 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:22:10 - [âœ”ï¸]: Translated 400 sents for fr-da\n",
      "DEBUG: 2025-05-09 13:22:10 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:26:42 - [âœ”ï¸]: Translated 400 sents for fr-de\n",
      "DEBUG: 2025-05-09 13:26:42 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:31:43 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:36:34 - [âœ”ï¸]: Translated 400 sents for fr-el\n",
      "DEBUG: 2025-05-09 13:36:35 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:39:17 - [âœ”ï¸]: Translated 400 sents for fr-es\n",
      "DEBUG: 2025-05-09 13:39:17 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:49:07 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:54:08 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:58:41 - [âœ”ï¸]: Translated 400 sents for fr-fi\n",
      "DEBUG: 2025-05-09 13:58:41 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:02:23 - [âœ”ï¸]: Translated 400 sents for fr-it\n",
      "DEBUG: 2025-05-09 14:02:23 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:07:12 - [âœ”ï¸]: Translated 400 sents for fr-nl\n",
      "DEBUG: 2025-05-09 14:07:12 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:10:56 - [âœ”ï¸]: Translated 400 sents for fr-pt\n",
      "DEBUG: 2025-05-09 14:10:56 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "!cat proc3.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a12cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pt', 'da')\n",
      "start 2025-05-09 19:19:38\n",
      "end 2025-05-09 19:28:39\n",
      "duration 541.1454193592072\n",
      "\n",
      "('sv', 'es')\n",
      "start 2025-05-09 21:21:34\n",
      "end 2025-05-09 21:24:14\n",
      "duration 160.7068531513214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "with open(join('tasks', 'proc3.jsonl'), 'r') as f:\n",
    "    logs = [json.loads(ln) for ln in f.readlines()]\n",
    "\n",
    "pairs = [('sv', 'es'), ('pt', 'da')]\n",
    "\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    if pair in pairs:\n",
    "        print(pair)\n",
    "        start = datetime.fromtimestamp(\n",
    "            log['start']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end = datetime.fromtimestamp(log['end']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('start', start)\n",
    "        print('end', end)\n",
    "        print('duration', log['end'] - log['start'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda0327",
   "metadata": {},
   "source": [
    "* For some cases, the structured logs from `proc3.jsonl` got the time right because the automtic retries exceeded OpenAI's limit of 2 retries and triggered the retries implemented by us, which restarted the timing.\n",
    "* However, in other cases, such as `pt-da`, we observe that the structured logs captured start time `19:19:38` which corresponds to the first try of OpenAI's code, not the last, hence the duration is longer than it really was.\n",
    "* Our goal is to now to go through proc1 to 3 logs and store the real start and end times. We lose precision as we aren't working with Unix timestamps anymore, however, since we mainly work with seconds, it should not make a big difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d02feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc2.log proc3.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" > tmp_proc2-3.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec0ab36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc2-3.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "441ab579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        pair2log_idx[pair] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14bd2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    idx = pair2log_idx[pair]\n",
    "    start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "    end = re.search(time_pat, logs[idx]).group(1)\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    start_unix = time.mktime(start.timetuple())\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    end_unix = time.mktime(end.timetuple())\n",
    "    pair2time[pair] = {'start': start_unix, 'end': end_unix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c35832f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# Number of translations generated by Proc2\n",
    "!ls tasks/proc2/europarl/gpt-4.1-2025-04-14/*.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a0e357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Number of translations generated by Proc 3\n",
    "!ls tasks/proc3/europarl-*/gpt-4.1-2025-04-14/*.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6826ab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc2_success = 44 \n",
    "proc3_success = 42\n",
    "len(pair2time) == proc2_success + proc3_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f81bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('proc2-3-europarl-gpt.json', 'w') as f:\n",
    "    json.dump(pair2time, fp=f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff62e26",
   "metadata": {},
   "source": [
    "* For Proc1, we need to make it a bit smarter, since Proc1 involved EuroParl, Opus100 and Flores+ as well, so we cannot use the pair directly as the sole key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85d45c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc1.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -A1 \"^(DEBUG)\" > tmp_proc1.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b5742db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc1.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954521d",
   "metadata": {},
   "source": [
    "* We exploit the fact that the logs are in chronological order\n",
    "* We know which Dataset was used with GPT4.1 first, second and third, so we can rely on the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9813daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        if pair not in pair2log_idx:\n",
    "            pair2log_idx[pair] = [i]\n",
    "        else:\n",
    "            pair2log_idx[pair].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02303959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    indices = pair2log_idx[pair]\n",
    "    pair2time[pair] = []\n",
    "    for idx in indices:\n",
    "        if logs[idx-1].startswith('DEBUG'):\n",
    "            start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "            end = re.search(time_pat, logs[idx]).group(1)\n",
    "            start = datetime.strptime(start, fmt)\n",
    "            start_unix = time.mktime(start.timetuple())\n",
    "            end = datetime.strptime(end, fmt)\n",
    "            end_unix = time.mktime(end.timetuple())\n",
    "            pair2time[pair].append({'start': start_unix, 'end': end_unix})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0af6c1",
   "metadata": {},
   "source": [
    "* Since Proc1 was successful throughout, we expect 3 time values for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0611e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = [len(times) == 3 for times in pair2time.values()]\n",
    "all(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec78def3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"task_id\": \"9549e927-4f63-4205-82bb-5c7ccabfe943\",\n",
      "    \"task_id\": \"fdbf6190-d061-4154-ae94-d7b11199d043\",\n",
      "    \"task_id\": \"fa628bc1-4f85-446d-9167-1a8d99ccc493\",\n"
     ]
    }
   ],
   "source": [
    "!cat tasks/proc1/europarl/gpt-4.1-2025-04-14/task.json | grep -P 'task_id'\n",
    "!cat tasks/proc1/flores_plus/gpt-4.1-2025-04-14/task.json | grep -P 'task_id'\n",
    "!cat tasks/proc1/opus-100/gpt-4.1-2025-04-14/task.json | grep -P 'task_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db216488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2025-05-07 13:55:45 - [ðŸ]: Starting task 9549e927-4f63-4205-82bb-5c7ccabfe943 on commit 66ef922\n",
      "INFO: 2025-05-07 15:01:53 - [ðŸ]: Starting task fa628bc1-4f85-446d-9167-1a8d99ccc493 on commit 66ef922\n",
      "INFO: 2025-05-07 15:41:35 - [ðŸ]: Starting task fdbf6190-d061-4154-ae94-d7b11199d043 on commit 66ef922\n"
     ]
    }
   ],
   "source": [
    "!cat proc1.log | grep -P \"Starting task (9549e927-|fdbf6190-|fa628bc1-)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856dca60",
   "metadata": {},
   "source": [
    "* So we infer that the first one is EuroParl, the second is Opus100 and the third is FloresPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34f9e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair2time_modified = {}\n",
    "for pair, times in pair2time.items():\n",
    "    pair2time_modified[pair] = {'ep': times[0], 'opus': times[1], 'flores': times[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d98aaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('proc1-gpt.json', 'w') as f:\n",
    "    json.dump(pair2time_modified, fp=f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126221fe",
   "metadata": {},
   "source": [
    "* For Proc 4 to 6, we can rely on structured logs as there were no automatic retries from OpenAI's side anymore.\n",
    "* We can still confirm as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "988760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc4.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" > tmp_proc4.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "001f0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc4.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76867064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        pair2log_idx[pair] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e269747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    idx = pair2log_idx[pair]\n",
    "    start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "    end = re.search(time_pat, logs[idx]).group(1)\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    start_unix = time.mktime(start.timetuple())\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    end_unix = time.mktime(end.timetuple())\n",
    "    pair2time[pair] = {'start': start_unix, 'end': end_unix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1338aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair2time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0bbc945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(join('tasks', 'proc4.jsonl'), 'r') as f:\n",
    "    logs = [json.loads(ln) for ln in f.readlines()]\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d4df21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_keys_1 = set(pair2time.keys())\n",
    "pair_keys_2 = set([f'{log[\"src_lang\"]}-{log[\"tgt_lang\"]}' for log in logs])\n",
    "pair_keys_1 == pair_keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ee8c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 0.79 s\n",
      "Min diff: -1.21 s\n",
      "Mean diff: -0.19 s\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    pair_key = '-'.join(pair)\n",
    "    structured_dur = log['end'] - log['start']\n",
    "    unstructured_dur = pair2time[pair_key]['end'] - pair2time[pair_key]['start']\n",
    "    diff = structured_dur - unstructured_dur\n",
    "    diffs.append(diff)\n",
    "\n",
    "print(f'Max diff: {max(diffs):.2f} s')\n",
    "print(f'Min diff: {min(diffs):.2f} s')\n",
    "print(f'Mean diff: {sum(diffs)/len(diffs):.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7d6fa",
   "metadata": {},
   "source": [
    "* Not too tragic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "940b4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da-de\n",
      "duration (structured log) 179.83 s\n",
      "duration (unstructured log) 180.00 s\n",
      "\n",
      "el-es\n",
      "duration (structured log) 155.98 s\n",
      "duration (unstructured log) 156.00 s\n",
      "\n",
      "es-nl\n",
      "duration (structured log) 155.98 s\n",
      "duration (unstructured log) 156.00 s\n",
      "\n",
      "fi-es\n",
      "duration (structured log) 153.05 s\n",
      "duration (unstructured log) 153.00 s\n",
      "\n",
      "nl-pt\n",
      "duration (structured log) 172.44 s\n",
      "duration (unstructured log) 172.00 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "selected_pairs = sample(list(pair_keys_1), 5)\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    pair_key = '-'.join(pair)\n",
    "    if pair_key in selected_pairs:\n",
    "        print(pair_key)\n",
    "        print('duration (structured log)', f'{log[\"end\"] - log[\"start\"]:.2f} s')\n",
    "        print('duration (unstructured log)', f'{pair2time[pair_key][\"end\"] - pair2time[pair_key][\"start\"]:.2f} s')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9903d5",
   "metadata": {},
   "source": [
    "## Preparing Files for Analysis\n",
    "* At the moment, all files are stored hierarchically in the `tasks` folder\n",
    "* This is not very convenient for analysis purposes, so we stored them all into a single folder preserve hierarchical information by filename prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f78e9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import json\n",
    "\n",
    "procs = [f'proc{i}' for i in range(1, 7)]\n",
    "\n",
    "long2short = {\n",
    "    'gpt-4.1-2025-04-14': 'gpt',\n",
    "    'deepl_document': 'deepl',\n",
    "    'opus-100': 'opus',\n",
    "    'europarl': 'ep',\n",
    "    'flores_plus': 'flores'\n",
    "}\n",
    "\n",
    "prefix2file = {}\n",
    "for p in procs:\n",
    "    folder = join('tasks', p)\n",
    "    folders = os.listdir(folder)\n",
    "    for fo in folders:\n",
    "        tl_folders = os.listdir(join(folder, fo))\n",
    "        for t in tl_folders:\n",
    "            tl_files = [f for f in os.listdir(join(folder, join(fo, t))) if f.endswith('.txt')]\n",
    "            for tf in tl_files:\n",
    "                pair = tf.replace('.txt', '')\n",
    "                if 'fail' in pair:\n",
    "                    pair = re.sub(r'_fail\\d+', '', pair)\n",
    "                if p in ['proc1', 'proc2', 'proc5', 'proc6']:\n",
    "                    sf = long2short[fo]\n",
    "                else:\n",
    "                    sf = long2short[fo.split('-')[0]]\n",
    "                st = long2short[t]\n",
    "                prefix = f'{sf}-{st}-{pair}'\n",
    "                prefix2file[prefix] = {'file': join(folder, join(fo, join(t, tf))), 'procedure': p}\n",
    "                with open(join('tasks', f'{p}.jsonl'), 'r') as fi:\n",
    "                    logs = [json.loads(ln) for ln in fi.readlines()]\n",
    "                for log in logs:\n",
    "                    log_pair = f'{log[\"src_lang\"]}-{log[\"tgt_lang\"]}'\n",
    "                    log_translator = log['translator']\n",
    "                    if log_pair == pair and log_translator==t:\n",
    "                        prefix2file[prefix]['log'] = log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "350bb58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 220 219\n"
     ]
    }
   ],
   "source": [
    "opus = 0\n",
    "flores = 0\n",
    "ep = 0\n",
    "deepl = 0\n",
    "gpt = 0\n",
    "for prefix, info in prefix2file.items():\n",
    "    p1 = prefix.split('-')[0]\n",
    "    p2 = prefix.split('-')[1]\n",
    "    if p2 == 'deepl':\n",
    "        deepl+=1\n",
    "    if p2 == 'gpt':\n",
    "        gpt+=1\n",
    "    if p1 == 'opus':\n",
    "        opus+=1\n",
    "    if p1 == 'ep':\n",
    "        ep+=1\n",
    "    if p1 == 'flores':\n",
    "        flores+=1\n",
    "\n",
    "\n",
    "print(opus, flores, ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e51db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 239\n"
     ]
    }
   ],
   "source": [
    "print(deepl, gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c3702",
   "metadata": {},
   "source": [
    "* 40 translation for OPUS, 20 DeepL, 20 GPT4.1\n",
    "* 220 translations for Flores+, 110 DeepL, 110 GPT4.1\n",
    "* 219 translations for Europarl, 110 DeepL, 109 GPT4.1 (because it kept failing `it-el`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f5f9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-gpt-de-en: 107.51s -> 252.00s\n",
      "opus-gpt-de-en: 107.51s -> 76.00s\n",
      "ep-gpt-en-de: 169.91s -> 194.00s\n",
      "opus-gpt-en-de: 169.91s -> 114.00s\n",
      "ep-gpt-da-en: 128.14s -> 142.00s\n",
      "opus-gpt-da-en: 128.14s -> 99.00s\n",
      "flores-gpt-da-en: 128.14s -> 129.00s\n",
      "ep-gpt-en-da: 186.58s -> 155.00s\n",
      "opus-gpt-en-da: 186.58s -> 137.00s\n",
      "ep-gpt-el-en: 220.27s -> 240.00s\n",
      "opus-gpt-el-en: 220.27s -> 98.00s\n",
      "ep-gpt-en-el: 599.17s -> 239.00s\n",
      "opus-gpt-en-el: 599.17s -> 217.00s\n",
      "flores-gpt-en-el: 599.17s -> 299.00s\n",
      "ep-gpt-pt-en: 169.84s -> 89.00s\n",
      "opus-gpt-pt-en: 169.84s -> 86.00s\n",
      "ep-gpt-en-pt: 187.91s -> 121.00s\n",
      "opus-gpt-en-pt: 187.91s -> 106.00s\n",
      "ep-gpt-sv-en: 133.35s -> 111.00s\n",
      "opus-gpt-sv-en: 133.35s -> 64.00s\n",
      "ep-gpt-en-sv: 134.15s -> 105.00s\n",
      "opus-gpt-en-sv: 134.15s -> 83.00s\n",
      "ep-gpt-es-en: 102.99s -> 116.00s\n",
      "opus-gpt-es-en: 102.99s -> 86.00s\n",
      "ep-gpt-en-es: 135.10s -> 159.00s\n",
      "opus-gpt-en-es: 135.10s -> 94.00s\n",
      "ep-gpt-fi-en: 156.83s -> 139.00s\n",
      "opus-gpt-fi-en: 156.83s -> 95.00s\n",
      "ep-gpt-en-fi: 223.18s -> 275.00s\n",
      "opus-gpt-en-fi: 223.18s -> 179.00s\n",
      "ep-gpt-fr-en: 121.60s -> 145.00s\n",
      "opus-gpt-fr-en: 121.60s -> 98.00s\n",
      "ep-gpt-en-fr: 137.28s -> 176.00s\n",
      "opus-gpt-en-fr: 137.28s -> 182.00s\n",
      "ep-gpt-it-en: 129.80s -> 153.00s\n",
      "opus-gpt-it-en: 129.80s -> 82.00s\n",
      "ep-gpt-en-it: 184.11s -> 177.00s\n",
      "opus-gpt-en-it: 184.11s -> 90.00s\n",
      "flores-gpt-en-it: 184.11s -> 185.00s\n",
      "ep-gpt-nl-en: 106.15s -> 190.00s\n",
      "opus-gpt-nl-en: 106.15s -> 63.00s\n",
      "ep-gpt-en-nl: 148.80s -> 207.00s\n",
      "opus-gpt-en-nl: 148.80s -> 137.00s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Update times\n",
    "with open('proc1-gpt.json', 'r') as f:\n",
    "    proc1_times = json.load(f)\n",
    "\n",
    "for pair, datasets in proc1_times.items():\n",
    "    for dataset, times in datasets.items():\n",
    "        key = f'{dataset}-gpt-{pair}'\n",
    "        diff1 = prefix2file[key]['log']['end'] - prefix2file[key]['log']['start']\n",
    "        diff2 = times['end'] - times['start']\n",
    "        prefix2file[key]['log']['start'] = times['start']\n",
    "        prefix2file[key]['log']['end'] = times['end']\n",
    "        if round(diff1) != round(diff2):\n",
    "            print(f'{key}: {diff1:.2f}s -> {diff2:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd8c70fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-gpt-da-fi: 588.78s -> 288.00s\n",
      "ep-gpt-de-el: 569.56s -> 269.00s\n",
      "ep-gpt-de-sv: 511.33s -> 192.00s\n",
      "ep-gpt-el-de: 230.38s -> 231.00s\n",
      "ep-gpt-el-es: 220.42s -> 221.00s\n",
      "ep-gpt-el-nl: 170.33s -> 171.00s\n",
      "ep-gpt-es-da: 243.37s -> 244.00s\n",
      "ep-gpt-es-el: 254.19s -> 255.00s\n",
      "ep-gpt-es-fr: 241.38s -> 242.00s\n",
      "ep-gpt-fi-nl: 300.36s -> 301.00s\n",
      "ep-gpt-fi-sv: 269.22s -> 270.00s\n",
      "ep-gpt-fr-da: 243.23s -> 244.00s\n",
      "ep-gpt-fr-el: 592.28s -> 291.00s\n",
      "ep-gpt-fr-fi: 1164.01s -> 273.00s\n",
      "ep-gpt-fr-it: 221.33s -> 222.00s\n",
      "ep-gpt-it-da: 868.21s -> 267.00s\n",
      "ep-gpt-it-de: 193.62s -> 193.00s\n",
      "ep-gpt-it-es: 182.50s -> 182.00s\n",
      "ep-gpt-it-pt: 210.34s -> 211.00s\n",
      "ep-gpt-nl-da: 562.67s -> 263.00s\n",
      "ep-gpt-nl-de: 535.17s -> 234.00s\n",
      "ep-gpt-nl-el: 820.63s -> 218.00s\n",
      "ep-gpt-nl-pt: 203.56s -> 203.00s\n",
      "ep-gpt-nl-sv: 293.55s -> 293.00s\n",
      "ep-gpt-pt-da: 541.15s -> 241.00s\n",
      "ep-gpt-pt-fi: 894.12s -> 291.00s\n",
      "ep-gpt-pt-sv: 234.47s -> 235.00s\n"
     ]
    }
   ],
   "source": [
    "with open('proc2-3-europarl-gpt.json') as f:\n",
    "    proc2_3_times = json.load(f)\n",
    "\n",
    "for pair, times in proc2_3_times.items():\n",
    "    key = f'ep-gpt-{pair}'\n",
    "    diff1 = prefix2file[key]['log']['end'] - prefix2file[key]['log']['start']\n",
    "    diff2 = times['end'] - times['start']\n",
    "    prefix2file[key]['log']['start'] = times['start']\n",
    "    prefix2file[key]['log']['end'] = times['end']\n",
    "    if round(diff1) != round(diff2):\n",
    "        print(f'{key}: {diff1:.2f}s -> {diff2:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e820041c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m], join(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(join(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix2file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "os.makedirs('translations', exist_ok=True)\n",
    "for prefix, info in prefix2file.items():\n",
    "    shutil.copy(info['file'], join('translations', f'{prefix}.txt'))\n",
    "\n",
    "with open(join('translations', 'info.json'), 'w') as f:\n",
    "    json.dump(prefix2file, file=f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = os.listdir('translations')\n",
    "len(check) == 480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4009914",
   "metadata": {},
   "source": [
    "## Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb90f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prefix2time = {}\n",
    "with open(join('translations', 'info.json'), 'r') as f:\n",
    "    prefix2file = json.load(f)\n",
    "\n",
    "for prefix, info in prefix2file.items():\n",
    "    prefix2time[prefix] = info['log']['end'] - info['log']['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc2-3-europarl-gpt.json') as f:\n",
    "    pair2time = json.load(f)\n",
    "\n",
    "for prefix in prefix2time:\n",
    "    if prefix.startswith('ep-gpt'):\n",
    "        info = prefix2file[prefix]\n",
    "        if info['procedure'] == 'proc2' or info['procedure'] == 'proc3':\n",
    "            pair = '-'.join([prefix.split('-')[2], prefix.split('-')[3]])\n",
    "            prefix2time[prefix] = pair2time[pair]['end'] - pair2time[pair]['start']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc1-gpt.json') as f:\n",
    "    pair2time = json.load(f)\n",
    "\n",
    "for prefix in prefix2time:\n",
    "    data, tl = prefix.split('-')[:2]\n",
    "    if tl == 'gpt':\n",
    "        info = prefix2file[prefix]\n",
    "        if info['procedure'] == 'proc1':\n",
    "            pair = '-'.join([prefix.split('-')[2], prefix.split('-')[3]])\n",
    "            prefix2time[prefix] = pair2time[pair][data]['end'] - pair2time[pair][data]['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for prefix in sorted(prefix2time, key=lambda x: prefix2time[x], reverse=True):\n",
    "    if 'gpt' in prefix:\n",
    "        print(prefix, f'{prefix2time[prefix]:.2f}')\n",
    "        times.append(prefix2time[prefix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(times)/len(times)\n",
    "print(f'Mean: {mean:.2f}s')\n",
    "print(f'Max: {max(times):.2f}s')\n",
    "print(f'Min: {min(times):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4046",
   "metadata": {},
   "source": [
    "## Alignment Scheduling\n",
    "* We can somewhat guess which translations have to be aligned again by checking if the number of outlines != 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import json\n",
    "with open(join('translations', 'info.json'), 'r') as f:\n",
    "    prefix2file = json.load(f)\n",
    "\n",
    "num = 0\n",
    "for prefix, info in prefix2file.items():\n",
    "    outlines = info['log']['out_lines']\n",
    "    if outlines != 400:\n",
    "        print(prefix, outlines)\n",
    "        num += 1\n",
    "\n",
    "print(f'{num} pairs likely need to be re-aligned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
