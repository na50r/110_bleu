{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcc926c",
   "metadata": {},
   "source": [
    "# Getting Preciser Timestamps\n",
    "* In proc 1 to 3, there were cases where GPT4.1 took longer than expected. \n",
    "* The structured logs in the JSONL file stored the start time when the method that calls the API was called and the end time when the response was received. \n",
    "* However, this did not account for the case of OpenAI's code doing the retries, so the retries where included in the time calculation.\n",
    "* In this notebook, we use the logs to extract start and end times that are more in line with the real translation time\n",
    "* We do this by looking at the `DEBUG` logs created by OpenAI's code, they contain an entry of when exactly a request was sent.\n",
    "* The end time we can obtain by our own logs, looking for `Translated X sents for src-tgt` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2895f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 2025-05-09 13:18:06 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:22:10 - [✔️]: Translated 400 sents for fr-da\n",
      "DEBUG: 2025-05-09 13:22:10 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:26:42 - [✔️]: Translated 400 sents for fr-de\n",
      "DEBUG: 2025-05-09 13:26:42 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:31:43 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:36:34 - [✔️]: Translated 400 sents for fr-el\n",
      "DEBUG: 2025-05-09 13:36:35 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:39:17 - [✔️]: Translated 400 sents for fr-es\n",
      "DEBUG: 2025-05-09 13:39:17 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:49:07 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 13:54:08 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 13:58:41 - [✔️]: Translated 400 sents for fr-fi\n",
      "DEBUG: 2025-05-09 13:58:41 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:02:23 - [✔️]: Translated 400 sents for fr-it\n",
      "DEBUG: 2025-05-09 14:02:23 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:07:12 - [✔️]: Translated 400 sents for fr-nl\n",
      "DEBUG: 2025-05-09 14:07:12 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:10:56 - [✔️]: Translated 400 sents for fr-pt\n",
      "DEBUG: 2025-05-09 14:10:56 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:15:46 - [✔️]: Translated 400 sents for fr-sv\n",
      "DEBUG: 2025-05-09 14:17:04 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:22:05 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:27:06 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:31:33 - [✔️]: Translated 400 sents for it-da\n",
      "DEBUG: 2025-05-09 14:31:33 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 14:34:46 - [✔️]: Translated 400 sents for it-de\n",
      "DEBUG: 2025-05-09 14:34:47 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:39:48 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:44:49 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:50:19 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 14:55:20 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:00:21 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:05:51 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:10:53 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:15:54 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:20:55 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 15:23:57 - [✔️]: Translated 400 sents for it-es\n",
      "DEBUG: 2025-05-09 15:23:57 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:28:58 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:33:59 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:39:30 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:44:31 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:49:32 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 15:55:02 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:00:03 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:05:04 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:10:05 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:13:11 - [✔️]: Translated 400 sents for it-fr\n",
      "DEBUG: 2025-05-09 16:13:11 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:17:37 - [✔️]: Translated 400 sents for it-nl\n",
      "DEBUG: 2025-05-09 16:17:37 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:21:08 - [✔️]: Translated 400 sents for it-pt\n",
      "DEBUG: 2025-05-09 16:21:08 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:26:07 - [✔️]: Translated 400 sents for it-sv\n",
      "DEBUG: 2025-05-09 16:28:04 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:33:04 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:37:27 - [✔️]: Translated 400 sents for nl-da\n",
      "DEBUG: 2025-05-09 16:37:27 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:42:28 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 16:46:22 - [✔️]: Translated 400 sents for nl-de\n",
      "DEBUG: 2025-05-09 16:46:22 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:51:23 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 16:56:25 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:00:03 - [✔️]: Translated 400 sents for nl-el\n",
      "DEBUG: 2025-05-09 17:00:03 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:03:38 - [✔️]: Translated 400 sents for nl-es\n",
      "DEBUG: 2025-05-09 17:03:38 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:08:39 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:13:40 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:19:11 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:19:28 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:24:29 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 17:26:07 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:30:57 - [✔️]: Translated 400 sents for nl-fi\n",
      "DEBUG: 2025-05-09 17:30:57 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:35:21 - [✔️]: Translated 400 sents for nl-fr\n",
      "DEBUG: 2025-05-09 17:35:22 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:39:50 - [✔️]: Translated 400 sents for nl-it\n",
      "DEBUG: 2025-05-09 17:39:50 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:43:13 - [✔️]: Translated 400 sents for nl-pt\n",
      "DEBUG: 2025-05-09 17:43:14 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 17:48:07 - [✔️]: Translated 400 sents for nl-sv\n",
      "DEBUG: 2025-05-09 19:19:38 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 19:24:39 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 19:28:40 - [✔️]: Translated 400 sents for pt-da\n",
      "DEBUG: 2025-05-09 19:28:40 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 19:33:11 - [✔️]: Translated 400 sents for pt-de\n",
      "DEBUG: 2025-05-09 19:33:11 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 19:38:12 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 19:43:13 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 19:48:44 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 19:53:33 - [✔️]: Translated 400 sents for pt-el\n",
      "DEBUG: 2025-05-09 19:53:33 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 19:57:10 - [✔️]: Translated 400 sents for pt-es\n",
      "DEBUG: 2025-05-09 19:57:10 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:02:11 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:07:13 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:12:04 - [✔️]: Translated 401 sents for pt-fi\n",
      "DEBUG: 2025-05-09 20:12:05 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:15:33 - [✔️]: Translated 400 sents for pt-fr\n",
      "DEBUG: 2025-05-09 20:15:33 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:19:14 - [✔️]: Translated 400 sents for pt-it\n",
      "DEBUG: 2025-05-09 20:19:14 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:22:58 - [✔️]: Translated 400 sents for pt-nl\n",
      "DEBUG: 2025-05-09 20:22:58 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:26:53 - [✔️]: Translated 400 sents for pt-sv\n",
      "DEBUG: 2025-05-09 20:27:10 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:30:57 - [✔️]: Translated 400 sents for sv-da\n",
      "DEBUG: 2025-05-09 20:30:58 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 20:35:26 - [✔️]: Translated 400 sents for sv-de\n",
      "DEBUG: 2025-05-09 20:35:26 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:40:27 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:45:28 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:50:59 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 20:56:00 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 21:01:01 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 21:06:31 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 21:11:32 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 21:16:33 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG: 2025-05-09 21:21:34 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:24:15 - [✔️]: Translated 400 sents for sv-es\n",
      "DEBUG: 2025-05-09 21:24:15 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:28:54 - [✔️]: Translated 399 sents for sv-fi\n",
      "DEBUG: 2025-05-09 21:28:55 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:32:13 - [✔️]: Translated 400 sents for sv-fr\n",
      "DEBUG: 2025-05-09 21:32:13 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:35:03 - [✔️]: Translated 400 sents for sv-it\n",
      "DEBUG: 2025-05-09 21:35:03 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:38:53 - [✔️]: Translated 400 sents for sv-nl\n",
      "DEBUG: 2025-05-09 21:38:53 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "INFO: 2025-05-09 21:41:08 - [✔️]: Translated 400 sents for sv-pt\n"
     ]
    }
   ],
   "source": [
    "!cat proc3.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a12cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pt', 'da')\n",
      "start 2025-05-09 19:19:38\n",
      "end 2025-05-09 19:28:39\n",
      "duration 541.1454193592072\n",
      "\n",
      "('sv', 'es')\n",
      "start 2025-05-09 21:21:34\n",
      "end 2025-05-09 21:24:14\n",
      "duration 160.7068531513214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "with open(join('tasks', 'proc3.jsonl'), 'r') as f:\n",
    "    logs = [json.loads(ln) for ln in f.readlines()]\n",
    "\n",
    "pairs = [('sv', 'es'), ('pt', 'da')]\n",
    "\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    if pair in pairs:\n",
    "        print(pair)\n",
    "        start = datetime.fromtimestamp(\n",
    "            log['start']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end = datetime.fromtimestamp(log['end']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('start', start)\n",
    "        print('end', end)\n",
    "        print('duration', log['end'] - log['start'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda0327",
   "metadata": {},
   "source": [
    "* For some cases, the structured logs from `proc3.jsonl` got the time right because the automtic retries exceeded OpenAI's limit of 2 retries and triggered the retries implemented by us, which restarted the timing.\n",
    "* However, in other cases, such as `pt-da`, we observe that the structured logs captured start time `19:19:38` which corresponds to the first try of OpenAI's code, not the last, hence the duration is longer than it really was.\n",
    "* Our goal is to now to go through proc1 to 3 logs and store the real start and end times. We lose precision since the timestamps are not UNIX timestamps but since we work with seconds anyway, that should not be a big issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d02feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc2.log proc3.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" > tmp_proc2-3.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec0ab36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc2-3.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "441ab579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        pair2log_idx[pair] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14bd2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    idx = pair2log_idx[pair]\n",
    "    start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "    end = re.search(time_pat, logs[idx]).group(1)\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    start_unix = time.mktime(start.timetuple())\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    end_unix = time.mktime(end.timetuple())\n",
    "    pair2time[pair] = {'start': start_unix, 'end': end_unix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6826ab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc2_success = 44\n",
    "proc3_success = 42\n",
    "len(pair2time) == proc2_success + proc3_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f81bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('proc2-3-europarl-gpt.json', 'w') as f:\n",
    "    json.dump(pair2time, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff62e26",
   "metadata": {},
   "source": [
    "* For Proc1, we need to make it a bit smarter, since Proc1 involved EuroParl, Opus100 and Flores+ as well, so we cannot use the pair directly as the sole key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85d45c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc1.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" > tmp_proc1.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b5742db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc1.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9813daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        if pair not in pair2log_idx:\n",
    "            pair2log_idx[pair] = [i]\n",
    "        else:\n",
    "            pair2log_idx[pair].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02303959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    indices = pair2log_idx[pair]\n",
    "    pair2time[pair] = []\n",
    "    for idx in indices:\n",
    "        if logs[idx-1].startswith('DEBUG'):\n",
    "            start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "            end = re.search(time_pat, logs[idx]).group(1)\n",
    "            start = datetime.strptime(start, fmt)\n",
    "            start_unix = time.mktime(start.timetuple())\n",
    "            end = datetime.strptime(end, fmt)\n",
    "            end_unix = time.mktime(end.timetuple())\n",
    "            pair2time[pair].append({'start': start_unix, 'end': end_unix})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0611e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = [len(times) == 3 for times in pair2time.values()]\n",
    "all(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbecf3",
   "metadata": {},
   "source": [
    "* Proc 1 task ids for GPT4.1 are:\n",
    "    * `9549e927-4f63-4205-82bb-5c7ccabfe943` europarl\n",
    "    * `fdbf6190-d061-4154-ae94-d7b11199d043` flores_plus\n",
    "    * `fa628bc1-4f85-446d-9167-1a8d99ccc493` opus-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db216488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2025-05-07 13:55:45 - [🏁]: Starting task 9549e927-4f63-4205-82bb-5c7ccabfe943 on commit 66ef922\n",
      "INFO: 2025-05-07 14:57:36 - [🏁]: Starting task ab9fc42b-61af-4534-afd6-3710eb1dbc52 on commit 66ef922\n",
      "INFO: 2025-05-07 15:01:53 - [🏁]: Starting task fa628bc1-4f85-446d-9167-1a8d99ccc493 on commit 66ef922\n",
      "INFO: 2025-05-07 15:38:48 - [🏁]: Starting task 4c5ffc31-c043-414d-aba1-270f9967f078 on commit 66ef922\n",
      "INFO: 2025-05-07 15:41:35 - [🏁]: Starting task fdbf6190-d061-4154-ae94-d7b11199d043 on commit 66ef922\n",
      "INFO: 2025-05-07 16:39:58 - [🏁]: Starting task 88538d1d-11fc-4356-9153-b105d4e26e30 on commit 66ef922\n"
     ]
    }
   ],
   "source": [
    "!cat proc1.log | grep -P \"Starting task\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856dca60",
   "metadata": {},
   "source": [
    "* So we infer that the first one is EuroParl, the second is Opus100 and the third is FloresPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34f9e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair2time_modified = {}\n",
    "for pair, times in pair2time.items():\n",
    "    pair2time_modified[pair] = {'ep': times[0], 'opus': times[1], 'flores': times[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d98aaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc1-gpt.json', 'w') as f:\n",
    "    json.dump(pair2time_modified, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126221fe",
   "metadata": {},
   "source": [
    "* For Proc 4 to 6, we can rely on structured logs as there were no automatic retries from OpenAI's side anymore.\n",
    "* We can still confirm as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "988760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat proc4.log | grep -P \"(Sending HTTP Request: POST https://api.openai.com/v1/chat/completions|Translated \\d+ sents for \\w\\w-\\w\\w)\" | grep -P -B1 \"(Translated \\d+ sents for \\w\\w-\\w\\w)\" > tmp_proc4.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "001f0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_proc4.log', 'r') as f:\n",
    "    logs = [ln for ln in f if ln.startswith('DEBUG') or ln.startswith('INFO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76867064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r\"Translated \\d+ sents for (\\w\\w-\\w\\w)\"\n",
    "pair2log_idx = {}\n",
    "for i, log in enumerate(logs):\n",
    "    if log.startswith('INFO'):\n",
    "        pair = re.search(pat, log).group(1)\n",
    "        pair2log_idx[pair] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e269747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pair2time = {}\n",
    "time_pat = r\"(\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\"\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "for pair in pair2log_idx:\n",
    "    idx = pair2log_idx[pair]\n",
    "    start = re.search(time_pat, logs[idx-1]).group(1)\n",
    "    end = re.search(time_pat, logs[idx]).group(1)\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    start_unix = time.mktime(start.timetuple())\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    end_unix = time.mktime(end.timetuple())\n",
    "    pair2time[pair] = {'start': start_unix, 'end': end_unix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1338aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair2time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0bbc945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(join('tasks', 'proc4.jsonl'), 'r') as f:\n",
    "    logs = [json.loads(ln) for ln in f.readlines()]\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d4df21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_keys_1 = set(pair2time.keys())\n",
    "pair_keys_2 = set([f'{log[\"src_lang\"]}-{log[\"tgt_lang\"]}' for log in logs])\n",
    "pair_keys_1 == pair_keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ee8c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 0.79 s\n",
      "Min diff: -1.21 s\n",
      "Mean diff: -0.19 s\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    pair_key = '-'.join(pair)\n",
    "    structured_dur = log['end'] - log['start']\n",
    "    unstructured_dur = pair2time[pair_key]['end'] - pair2time[pair_key]['start']\n",
    "    diff = structured_dur - unstructured_dur\n",
    "    diffs.append(diff)\n",
    "\n",
    "print(f'Max diff: {max(diffs):.2f} s')\n",
    "print(f'Min diff: {min(diffs):.2f} s')\n",
    "print(f'Mean diff: {sum(diffs)/len(diffs):.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7d6fa",
   "metadata": {},
   "source": [
    "* Not too tragic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "940b4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el-fr\n",
      "duration (structured log) 203.13 s\n",
      "duration (unstructured log) 203.00 s\n",
      "\n",
      "es-sv\n",
      "duration (structured log) 192.00 s\n",
      "duration (unstructured log) 192.00 s\n",
      "\n",
      "it-da\n",
      "duration (structured log) 160.44 s\n",
      "duration (unstructured log) 161.00 s\n",
      "\n",
      "pt-nl\n",
      "duration (structured log) 150.94 s\n",
      "duration (unstructured log) 151.00 s\n",
      "\n",
      "sv-el\n",
      "duration (structured log) 261.04 s\n",
      "duration (unstructured log) 261.00 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "selected_pairs = sample(list(pair_keys_1), 5)\n",
    "for log in logs:\n",
    "    pair = (log['src_lang'], log['tgt_lang'])\n",
    "    pair_key = '-'.join(pair)\n",
    "    if pair_key in selected_pairs:\n",
    "        print(pair_key)\n",
    "        print('duration (structured log)', f'{log[\"end\"] - log[\"start\"]:.2f} s')\n",
    "        print('duration (unstructured log)', f'{pair2time[pair_key][\"end\"] - pair2time[pair_key][\"start\"]:.2f} s')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9903d5",
   "metadata": {},
   "source": [
    "## Preparing Files for Analysis\n",
    "* At the moment, all files are stored hierarchically in the `tasks` folder\n",
    "* This is not very convenient for analysis purposes, so we stored them all into a single folder preserve hierarchical information by filename prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78e9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import json\n",
    "\n",
    "procs = [f'proc{i}' for i in range(1, 7)]\n",
    "\n",
    "long2short = {\n",
    "    'gpt-4.1-2025-04-14': 'gpt',\n",
    "    'deepl_document': 'deepl',\n",
    "    'opus-100': 'opus',\n",
    "    'europarl': 'ep',\n",
    "    'flores_plus': 'flores'\n",
    "}\n",
    "\n",
    "prefix2file = {}\n",
    "for p in procs:\n",
    "    folder = join('tasks', p)\n",
    "    folders = os.listdir(folder)\n",
    "    for f in folders:\n",
    "        tl_folders = os.listdir(join(folder, f))\n",
    "        for t in tl_folders:\n",
    "            tl_files = [f for f in os.listdir(join(folder, join(f, t))) if f.endswith('.txt')]\n",
    "            for tf in tl_files:\n",
    "                pair = tf.replace('.txt', '')\n",
    "                if 'fail' in pair:\n",
    "                    pair = re.sub(r'_fail\\d+', '', pair)\n",
    "                if p in ['proc1', 'proc2', 'proc5', 'proc6']:\n",
    "                    sf = long2short[f]\n",
    "                else:\n",
    "                    sf = long2short[f.split('-')[0]]\n",
    "                st = long2short[t]\n",
    "                prefix = f'{sf}-{st}-{pair}'\n",
    "                prefix2file[prefix] = {'file': join(folder, join(f, join(t, tf))), 'procedure': p}\n",
    "                with open(join('tasks', f'{p}.jsonl'), 'r') as fi:\n",
    "                    logs = [json.loads(ln) for ln in fi.readlines()]\n",
    "                for log in logs:\n",
    "                    log_pair = f'{log[\"src_lang\"]}-{log[\"tgt_lang\"]}'\n",
    "                    if log_pair == pair:\n",
    "                        prefix2file[prefix]['log'] = log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350bb58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 220 219\n"
     ]
    }
   ],
   "source": [
    "opus = 0\n",
    "flores = 0\n",
    "ep = 0\n",
    "for prefix, info in prefix2file.items():\n",
    "    p = prefix.split('-')[0]\n",
    "    if 'opus' == p:\n",
    "        opus += 1\n",
    "    if 'flores' == p:\n",
    "        flores += 1\n",
    "    if 'ep' == p:\n",
    "        ep += 1\n",
    "\n",
    "print(opus, flores, ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c3702",
   "metadata": {},
   "source": [
    "* 40 translation for OPUS, 20 DeepL, 20 GPT4.1\n",
    "* 220 translations for Flores+, 110 DeepL, 110 GPT4.1\n",
    "* 219 translations for Europarl, 110 DeepL, 109 GPT4.1 (because it kept failing `it-el`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e820041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "os.makedirs('translations', exist_ok=True)\n",
    "for prefix, info in prefix2file.items():\n",
    "    shutil.copy(info['file'], join('translations', f'{prefix}.txt'))\n",
    "\n",
    "with open(join('translations', 'info.json'), 'w') as f:\n",
    "    json.dump(prefix2file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e39e170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = os.listdir('translations')\n",
    "len(check) == 480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4009914",
   "metadata": {},
   "source": [
    "## Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb90f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prefix2time = {}\n",
    "with open(join('translations', 'info.json'), 'r') as f:\n",
    "    prefix2file = json.load(f)\n",
    "\n",
    "for prefix, info in prefix2file.items():\n",
    "    prefix2time[prefix] = info['log']['end'] - info['log']['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc2-3-europarl-gpt.json') as f:\n",
    "    pair2time = json.load(f)\n",
    "\n",
    "for prefix in prefix2time:\n",
    "    if prefix.startswith('ep-gpt'):\n",
    "        info = prefix2file[prefix]\n",
    "        if info['procedure'] == 'proc2' or info['procedure'] == 'proc3':\n",
    "            pair = '-'.join([prefix.split('-')[2], prefix.split('-')[3]])\n",
    "            prefix2time[prefix] = pair2time[pair]['end'] - pair2time[pair]['start']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "331f596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc1-gpt.json') as f:\n",
    "    pair2time = json.load(f)\n",
    "\n",
    "for prefix in prefix2time:\n",
    "    data, tl = prefix.split('-')[:2]\n",
    "    if tl == 'gpt':\n",
    "        info = prefix2file[prefix]\n",
    "        if info['procedure'] == 'proc1':\n",
    "            pair = '-'.join([prefix.split('-')[2], prefix.split('-')[3]])\n",
    "            prefix2time[prefix] = pair2time[pair][data]['end'] - pair2time[pair][data]['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8caf2534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-gpt-fi-nl 301.00\n",
      "flores-gpt-en-el 299.00\n",
      "ep-gpt-it-sv 299.00\n",
      "flores-gpt-nl-el 295.98\n",
      "flores-gpt-da-fi 295.43\n",
      "ep-gpt-nl-sv 293.00\n",
      "ep-gpt-fr-el 291.00\n",
      "ep-gpt-pt-fi 291.00\n",
      "ep-gpt-fr-sv 290.00\n",
      "ep-gpt-nl-fi 290.00\n",
      "ep-gpt-fr-nl 289.00\n",
      "ep-gpt-pt-el 289.00\n",
      "flores-gpt-el-fi 288.84\n",
      "ep-gpt-da-fi 288.00\n",
      "ep-gpt-it-fi 285.98\n",
      "flores-gpt-fr-de 284.79\n",
      "ep-gpt-el-da 281.00\n",
      "flores-gpt-de-fr 280.56\n",
      "ep-gpt-da-el 279.00\n",
      "ep-gpt-el-it 279.00\n",
      "ep-gpt-sv-fi 279.00\n",
      "ep-gpt-en-fi 275.00\n",
      "flores-gpt-it-el 274.76\n",
      "ep-gpt-fr-fi 273.00\n",
      "ep-gpt-fr-de 272.00\n",
      "ep-gpt-pt-de 271.00\n",
      "ep-gpt-fi-sv 270.00\n",
      "ep-gpt-de-el 269.00\n",
      "ep-gpt-sv-el 268.01\n",
      "ep-gpt-nl-it 268.00\n",
      "ep-gpt-sv-de 268.00\n",
      "ep-gpt-it-da 267.00\n",
      "ep-gpt-it-nl 266.00\n",
      "flores-gpt-es-el 264.90\n",
      "ep-gpt-nl-fr 264.00\n",
      "flores-gpt-fr-el 263.81\n",
      "ep-gpt-fi-el 263.32\n",
      "flores-gpt-el-da 263.09\n",
      "ep-gpt-nl-da 263.00\n",
      "flores-gpt-sv-el 261.04\n",
      "flores-gpt-fi-el 259.55\n",
      "ep-gpt-el-fi 257.00\n",
      "flores-gpt-pt-de 255.99\n",
      "ep-gpt-es-el 255.00\n",
      "ep-gpt-de-en 252.00\n",
      "flores-gpt-pt-fi 251.67\n",
      "ep-gpt-da-nl 245.00\n",
      "ep-gpt-es-fi 245.00\n",
      "flores-gpt-pt-da 244.79\n",
      "ep-gpt-es-da 244.00\n",
      "ep-gpt-fr-da 244.00\n",
      "flores-gpt-el-de 243.45\n",
      "ep-gpt-fi-da 243.00\n",
      "ep-gpt-es-fr 242.00\n",
      "ep-gpt-fi-de 241.00\n",
      "ep-gpt-pt-da 241.00\n",
      "ep-gpt-el-en 240.00\n",
      "ep-gpt-en-el 239.00\n",
      "ep-gpt-el-pt 237.00\n",
      "ep-gpt-pt-sv 235.00\n",
      "ep-gpt-nl-de 234.00\n",
      "flores-gpt-nl-fi 233.55\n",
      "ep-gpt-fi-it 233.00\n",
      "ep-gpt-da-it 232.00\n",
      "ep-gpt-el-de 231.00\n",
      "flores-gpt-fi-it 230.42\n",
      "ep-gpt-sv-nl 230.00\n",
      "ep-gpt-de-fr 228.00\n",
      "ep-gpt-sv-da 227.00\n",
      "ep-gpt-fr-pt 224.00\n",
      "ep-gpt-pt-nl 224.00\n",
      "flores-gpt-en-fi 223.00\n",
      "ep-gpt-fr-it 222.00\n",
      "ep-gpt-el-es 221.00\n",
      "ep-gpt-pt-it 221.00\n",
      "flores-gpt-el-en 220.00\n",
      "ep-gpt-el-fr 220.00\n",
      "ep-gpt-fi-es 220.00\n",
      "flores-gpt-de-it 219.26\n",
      "ep-gpt-da-es 218.00\n",
      "ep-gpt-nl-el 218.00\n",
      "flores-gpt-fr-sv 217.76\n",
      "opus-gpt-en-el 217.00\n",
      "ep-gpt-pt-es 217.00\n",
      "flores-gpt-sv-fi 216.89\n",
      "flores-gpt-es-fi 216.40\n",
      "ep-gpt-es-sv 216.00\n",
      "flores-gpt-it-de 215.52\n",
      "ep-gpt-es-nl 215.00\n",
      "ep-gpt-nl-es 215.00\n",
      "ep-gpt-da-pt 214.00\n",
      "flores-gpt-fi-pt 212.31\n",
      "ep-gpt-de-pt 211.00\n",
      "ep-gpt-it-pt 211.00\n",
      "ep-gpt-de-nl 209.00\n",
      "flores-gpt-fr-nl 208.81\n",
      "flores-gpt-de-fi 208.55\n",
      "ep-gpt-pt-fr 208.00\n",
      "flores-gpt-da-el 207.83\n",
      "flores-gpt-es-da 207.11\n",
      "ep-gpt-en-nl 207.00\n",
      "ep-gpt-fi-fr 206.00\n",
      "ep-gpt-de-fi 205.00\n",
      "flores-gpt-es-fr 203.84\n",
      "flores-gpt-el-fr 203.13\n",
      "ep-gpt-nl-pt 203.00\n",
      "ep-gpt-da-sv 202.00\n",
      "ep-gpt-de-es 202.00\n",
      "flores-gpt-it-fi 201.88\n",
      "flores-gpt-fr-fi 201.05\n",
      "ep-gpt-el-sv 201.00\n",
      "ep-gpt-fi-pt 200.00\n",
      "ep-gpt-sv-fr 198.00\n",
      "flores-gpt-fi-fr 197.39\n",
      "flores-gpt-it-sv 196.79\n",
      "flores-gpt-fi-da 195.91\n",
      "flores-gpt-pt-el 195.47\n",
      "ep-gpt-en-de 194.00\n",
      "ep-gpt-es-de 194.00\n",
      "flores-gpt-nl-fr 193.98\n",
      "ep-gpt-it-de 193.00\n",
      "ep-gpt-de-sv 192.00\n",
      "flores-gpt-es-sv 192.00\n",
      "ep-gpt-nl-en 190.00\n",
      "flores-gpt-el-it 189.27\n",
      "flores-gpt-en-pt 188.00\n",
      "flores-gpt-en-da 187.00\n",
      "ep-gpt-it-fr 186.00\n",
      "flores-gpt-pt-sv 185.69\n",
      "flores-gpt-en-it 185.00\n",
      "flores-gpt-sv-nl 183.57\n",
      "flores-gpt-de-el 183.20\n",
      "opus-gpt-en-fr 182.00\n",
      "ep-gpt-it-es 182.00\n",
      "flores-gpt-pt-fr 180.12\n",
      "flores-gpt-da-de 179.83\n",
      "flores-gpt-nl-da 179.31\n",
      "opus-gpt-en-fi 179.00\n",
      "flores-gpt-sv-it 178.86\n",
      "ep-gpt-de-da 178.00\n",
      "ep-gpt-en-it 177.00\n",
      "ep-gpt-en-fr 176.00\n",
      "ep-gpt-de-it 173.00\n",
      "flores-gpt-nl-pt 172.44\n",
      "flores-gpt-fi-de 172.21\n",
      "flores-gpt-el-nl 171.07\n",
      "ep-gpt-el-nl 171.00\n",
      "flores-gpt-da-fr 170.69\n",
      "flores-gpt-en-de 170.00\n",
      "flores-gpt-pt-en 170.00\n",
      "ep-gpt-sv-it 170.00\n",
      "flores-gpt-sv-de 169.42\n",
      "flores-gpt-da-pt 169.28\n",
      "flores-gpt-nl-sv 169.08\n",
      "flores-gpt-es-it 167.13\n",
      "ep-gpt-da-fr 167.00\n",
      "flores-gpt-de-es 164.79\n",
      "flores-gpt-fi-sv 163.30\n",
      "flores-gpt-es-de 162.92\n",
      "flores-gpt-fi-nl 162.21\n",
      "ep-gpt-fr-es 162.00\n",
      "ep-gpt-sv-es 161.00\n",
      "flores-gpt-it-da 160.44\n",
      "flores-gpt-el-sv 160.37\n",
      "flores-gpt-nl-it 160.16\n",
      "flores-gpt-de-pt 159.77\n",
      "ep-gpt-en-es 159.00\n",
      "ep-gpt-es-it 159.00\n",
      "flores-gpt-fi-en 157.00\n",
      "ep-gpt-da-de 157.00\n",
      "flores-gpt-el-es 155.98\n",
      "flores-gpt-es-nl 155.98\n",
      "flores-gpt-it-pt 155.24\n",
      "ep-gpt-en-da 155.00\n",
      "flores-gpt-el-pt 154.06\n",
      "flores-gpt-fi-es 153.05\n",
      "ep-gpt-it-en 153.00\n",
      "flores-gpt-fr-es 151.46\n",
      "flores-gpt-pt-nl 150.94\n",
      "flores-gpt-en-nl 149.00\n",
      "ep-gpt-es-pt 146.00\n",
      "flores-gpt-nl-de 145.81\n",
      "ep-gpt-fr-en 145.00\n",
      "flores-gpt-de-da 144.30\n",
      "flores-gpt-sv-es 143.80\n",
      "flores-gpt-da-es 142.83\n",
      "ep-gpt-da-en 142.00\n",
      "flores-gpt-sv-fr 139.43\n",
      "ep-gpt-fi-en 139.00\n",
      "flores-gpt-fr-it 138.62\n",
      "flores-gpt-pt-it 138.28\n",
      "flores-gpt-it-nl 137.13\n",
      "flores-gpt-de-sv 137.09\n",
      "flores-gpt-en-fr 137.00\n",
      "opus-gpt-en-da 137.00\n",
      "opus-gpt-en-nl 137.00\n",
      "flores-gpt-en-es 135.00\n",
      "ep-gpt-sv-pt 135.00\n",
      "flores-gpt-en-sv 134.00\n",
      "flores-gpt-sv-en 133.00\n",
      "flores-gpt-fr-da 130.24\n",
      "flores-gpt-it-en 130.00\n",
      "flores-gpt-sv-pt 129.70\n",
      "flores-gpt-da-en 129.00\n",
      "flores-gpt-fr-en 122.00\n",
      "ep-gpt-en-pt 121.00\n",
      "flores-gpt-es-pt 120.22\n",
      "flores-gpt-fr-pt 119.97\n",
      "flores-gpt-de-nl 119.63\n",
      "flores-gpt-it-es 118.24\n",
      "flores-gpt-da-nl 117.00\n",
      "ep-gpt-es-en 116.00\n",
      "flores-gpt-sv-da 115.68\n",
      "opus-gpt-en-de 114.00\n",
      "ep-gpt-sv-en 111.00\n",
      "flores-gpt-it-fr 110.10\n",
      "flores-gpt-da-sv 109.09\n",
      "flores-gpt-de-en 108.00\n",
      "flores-gpt-nl-en 106.00\n",
      "opus-gpt-en-pt 106.00\n",
      "flores-gpt-da-it 105.22\n",
      "ep-gpt-en-sv 105.00\n",
      "flores-gpt-es-en 103.00\n",
      "flores-gpt-nl-es 99.73\n",
      "opus-gpt-da-en 99.00\n",
      "opus-gpt-el-en 98.00\n",
      "opus-gpt-fr-en 98.00\n",
      "opus-gpt-fi-en 95.00\n",
      "opus-gpt-en-es 94.00\n",
      "flores-gpt-pt-es 92.87\n",
      "opus-gpt-en-it 90.00\n",
      "ep-gpt-pt-en 89.00\n",
      "opus-gpt-es-en 86.00\n",
      "opus-gpt-pt-en 86.00\n",
      "opus-gpt-en-sv 83.00\n",
      "opus-gpt-it-en 82.00\n",
      "opus-gpt-de-en 76.00\n",
      "opus-gpt-sv-en 64.00\n",
      "opus-gpt-nl-en 63.00\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for prefix in sorted(prefix2time, key=lambda x: prefix2time[x], reverse=True):\n",
    "    if 'gpt' in prefix:\n",
    "        print(prefix, f'{prefix2time[prefix]:.2f}')\n",
    "        times.append(prefix2time[prefix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c1a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 192.07s\n",
      "Max: 301.00s\n",
      "Min: 63.00s\n"
     ]
    }
   ],
   "source": [
    "mean = sum(times)/len(times)\n",
    "print(f'Mean: {mean:.2f}s')\n",
    "print(f'Max: {max(times):.2f}s')\n",
    "print(f'Min: {min(times):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4046",
   "metadata": {},
   "source": [
    "## Alignment Scheduling\n",
    "* We can somewhat guess which translations have to be aligned again by checking if the number of outlines != 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "859f65bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-gpt-da-it 399\n",
      "ep-gpt-de-fi 399\n",
      "ep-gpt-el-it 399\n",
      "ep-gpt-el-nl 399\n",
      "ep-gpt-es-el 399\n",
      "ep-gpt-es-nl 399\n",
      "ep-gpt-fi-it 399\n",
      "ep-gpt-fi-nl 399\n",
      "ep-gpt-pt-fi 401\n",
      "ep-gpt-sv-fi 399\n",
      "flores-gpt-es-el 402\n",
      "flores-gpt-fi-da 402\n",
      "flores-gpt-fi-el 402\n",
      "flores-gpt-fi-es 402\n",
      "flores-gpt-fi-nl 402\n",
      "flores-gpt-fi-pt 402\n",
      "flores-gpt-it-fr 402\n",
      "ep-gpt-it-fi 399\n",
      "18 pairs need to be re-aligned\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import json\n",
    "with open(join('translations', 'info.json'), 'r') as f:\n",
    "    prefix2file = json.load(f)\n",
    "\n",
    "num = 0\n",
    "for prefix, info in prefix2file.items():\n",
    "    outlines = info['log']['out_lines']\n",
    "    if outlines != 400:\n",
    "        print(prefix, outlines)\n",
    "        num += 1\n",
    "\n",
    "print(f'{num} pairs need to be re-aligned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
